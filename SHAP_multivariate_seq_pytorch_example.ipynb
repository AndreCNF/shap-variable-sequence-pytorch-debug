{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting SHAP with multivariate sequential data, on PyTorch\n",
    "---\n",
    "\n",
    "Although SHAP works pretty great for most use cases, providing realistic and well founded interpretability to machine learning models, it doesn't seem to be able to handle multivariate sequential data. The issue appears that it doesn't successfully filter out the padding values (which are needed to form tensors when the data has variable sequence length).\n",
    "\n",
    "This notebook serves as a simple example to illustrate and debug SHAP for this multivariate sequential data scenarios, using the PyTorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RrWE9R_Nkl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd              # Pandas to handle the data in dataframes\n",
    "import numpy as np               # Math operations with NumPy\n",
    "import torch                     # PyTorch to create and apply deep learning models\n",
    "import shap                      # Model-agnostic interpretability package inspired on Shapley values\n",
    "import utils                     # Contains auxiliary functions\n",
    "from Time_Series_Dataset import Time_Series_Dataset # Dataset subclass which allows the creation of Dataset objects\n",
    "from NeuralNetwork import NeuralNetwork # Import the neural network model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging packages\n",
    "import pixiedust                 # Debugging in Jupyter Notebook cells\n",
    "import time                      # Calculate code execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_data = np.array([[0, 0, 23, 284, 70, 5, 0],\n",
    "                     [0, 1, 24, 270, 73, 5, 0],\n",
    "                     [0, 2, 22, 290, 71, 5, 0],\n",
    "                     [0, 3, 20, 288, 65, 4, 1],\n",
    "                     [0, 4, 21, 297, 64, 4, 1],\n",
    "                     [1, 0, 25, 300, 76, 5, 0],\n",
    "                     [1, 1, 19, 283, 70, 5, 0],\n",
    "                     [1, 2, 19, 306, 59, 5, 1],\n",
    "                     [1, 3, 18, 298, 55, 3, 1],\n",
    "                     [2, 0, 20, 250, 70, 5, 0],\n",
    "                     [2, 1, 20, 254, 68, 4, 1],\n",
    "                     [2, 2, 19, 244, 70, 3, 1],\n",
    "                     [3, 0, 27, 264, 78, 4, 0],\n",
    "                     [3, 1, 22, 293, 67, 4, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df = pd.DataFrame(dmy_data, columns=['subject_id', 'ts', 'Var0', 'Var1', 'Var2', 'Var3', 'label'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of used features\n",
    "dmy_cols = list(dmy_df.columns)\n",
    "\n",
    "# Remove features that aren't used by the model to predict the label\n",
    "for unused_feature in ['subject_id', 'ts', 'label']:\n",
    "    dmy_cols.remove(unused_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model parameters and constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network and dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patients = dmy_df.subject_id.nunique()     # Total number of patients\n",
    "n_inputs = len(dmy_df.columns)               # Number of input features\n",
    "n_hidden = 2                                 # Number of hidden units\n",
    "n_outputs = 1                                # Number of outputs\n",
    "n_layers = 1                                 # Number of LSTM layers\n",
    "p_dropout = 0.2                              # Probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3                                  # Number of patients in a mini batch\n",
    "n_epochs = 50                                   # Number of epochs\n",
    "lr = 0.001                                      # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length dictionary\n",
    "\n",
    "(number of temporal events) of each sequence (patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_df = dmy_df.groupby('subject_id').ts.count().to_frame().sort_values(by='ts', ascending=False)\n",
    "seq_len_dict = dict([(idx, val[0]) for idx, val in list(zip(seq_len_df.index, seq_len_df.values))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df = utils.normalize_data(dmy_df, see_progress=False)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict, n_patients, n_inputs, padding_value=padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Time_Series_Dataset(data, dmy_norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for SHAP debugging purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader = utils.create_train_sets(dataset, validation_ratio=0.25, batch_size=batch_size, \n",
    "                                                           get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model (removing the two identifier columns and the labels from the input size)\n",
    "model = NeuralNetwork(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = utils.train(model, train_dataloader, val_dataloader, seq_len_dict, batch_size, n_epochs, \n",
    "                    lr, model_path='models/', padding_value=padding_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability / SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model is in evaluation mode, with dropout turned off\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = dataset.X, dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, x_lengths = utils.sort_by_seq_len(features, seq_len_dict, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview of the model's output for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = model(features[:, :, 2:].float(), x_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove indeces that correspond to outputs in padded data\n",
    "real_idx = [idx for n_subject in range(features.shape[0])\n",
    "            for idx in range(n_subject*features.shape[1], n_subject*features.shape[1]+x_lengths[n_subject])]\n",
    "real_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeces at the end of each sequence\n",
    "final_seq_idx = [n_subject*features.shape[1]+x_lengths[n_subject]-1 for n_subject in range(features.shape[0])]\n",
    "final_seq_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore outputs from padded data\n",
    "ref_output = ref_output[real_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series([float(x) for x in list(ref_output.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the important features and model output for the current patient\n",
    "dmy_df.assign(output=ref_output_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard SHAP library, as with the line of code bellow, doesn't work well as the model needs to know the true sequence length (as an argument of its feedforward method). Otherwise, it assumes that the paddings are real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, features[:, :, 2:].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(features[:, :, 2:].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.rnn.input_size), features=features[:, :, 2:].contiguous().view(-1, model.rnn.input_size).numpy(), feature_names=dmy_cols, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "subject = 0\n",
    "\n",
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[features[subject, 0, 0].item()]\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[subject, :seq_len], features=features[subject, :seq_len, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = model(features[subject, :, 2:].float().unsqueeze(0), [x_lengths[subject]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series([float(x) for x in list(ref_output.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the important features and model output for the current patient\n",
    "dmy_df[dmy_df.subject_id == subject].reset_index().drop(columns='index').assign(output=ref_output_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "ts = 3\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[subject, ts], features=features[subject, ts, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the adapted version of the SHAP library, as with the line of code bellow, makes the relative feature importance make sense (\"Var1\" doesn't relate at all with the output, while \"Var2\" and \"Var3\" are indeed the most relevant features). However, the contribution scores still aren't properly calculated, as they don't add up to the actual output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, features[:, :, 2:].float(), feedforward_args=[x_lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(features[:, :, 2:].float(), \n",
    "                                    feedforward_args=[x_lengths, x_lengths],\n",
    "                                    var_seq_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.rnn.input_size), features=features[:, :, 2:].contiguous().view(-1, model.rnn.input_size).numpy(), feature_names=dmy_cols, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "subject = 0\n",
    "\n",
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[features[subject, 0, 0].item()]\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[subject, :seq_len], features=features[subject, :seq_len, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = model(features[subject, :, 2:].float().unsqueeze(0), [x_lengths[subject]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series([float(x) for x in list(ref_output.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the important features and model output for the current patient\n",
    "dmy_df[dmy_df.subject_id == subject].reset_index().drop(columns='index').assign(output=ref_output_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "ts = 3\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[subject, ts], features=features[subject, ts, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, the problem might be that **SHAP is still letting in padding values**, as if they were realistic values. It seems that it could be due to the use of all the data, in the right side of the multiplication in the definition of the phi variable (contribution score), i.e. when we have something like `phis[l][j] = ... * (x[l] - data[l])`. In my adapted version of the SHAP package, this corresponds to lines 237 and 241."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "explainer = shap.GradientExplainer(model, features[:, :, 2:].float(), feedforward_args=[x_lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(features[:, :, 2:].float(), feedforward_args=[x_lengths], var_seq_len=True, see_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.rnn.input_size), features=features[:, :, 2:].contiguous().view(-1, model.rnn.input_size).numpy(), feature_names=dmy_cols, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "subject = 0\n",
    "\n",
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[features[subject, 0, 0].item()]\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(0.5, shap_values[subject, :seq_len], features=features[subject, :seq_len, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = model(features[subject, :, 2:].float().unsqueeze(0), [x_lengths[subject]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series([float(x) for x in list(ref_output.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the important features and model output for the current patient\n",
    "dmy_df[dmy_df.subject_id == subject].reset_index().drop(columns='index').assign(output=ref_output_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "ts = 3\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(0.5, shap_values[subject, ts], features=features[subject, ts, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient explainer, similarly to the deep explainer, doesn't calculate the contributions correctly, as it should add up to the output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will be used in the kernel explainer, converting a dataframe object into the model's output\n",
    "def f(data, hidden_state=None):\n",
    "    # Make sure the data is of type float\n",
    "    data = torch.from_numpy(data).unsqueeze(0).float()\n",
    "    \n",
    "    # Calculate the output\n",
    "    output = model(data, hidden_state=hidden_state)\n",
    "    \n",
    "    return output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "outputs = f(dmy_norm_df.values[:, 2:-1])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(f, dmy_norm_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(dmy_norm_df.values, isRNN=True, model_obj=model, l1_reg='aic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.rnn.input_size), features=features[:, :, 2:].contiguous().view(-1, model.rnn.input_size).numpy(), feature_names=dmy_cols, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "subject = 0\n",
    "\n",
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[features[subject, 0, 0].item()]\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[subject, :seq_len], features=features[subject, :seq_len, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = model(features[subject, :, 2:].float().unsqueeze(0), [x_lengths[subject]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series([float(x) for x in list(ref_output.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the important features and model output for the current patient\n",
    "dmy_df[dmy_df.subject_id == subject].reset_index().drop(columns='index').assign(output=ref_output_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "ts = 3\n",
    "\n",
    "# Plot the explanation of the predictions for one subject\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[subject, ts], features=features[subject, ts, 2:].numpy(), feature_names=dmy_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **the modified kernel explainer seems to calculate the features' contributions with acceptable accuracy** (the sums after the first instance don't match precisely with the real output values, but they're very close to them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_debug",
   "language": "python",
   "name": "shap_debug"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
